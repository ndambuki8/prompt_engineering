{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20735697",
   "metadata": {},
   "source": [
    "## Conversational App for Itinerary Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#from langchain import HuggingFaceHub\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "#retrieving api key\n",
    "key=os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c385b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagePlaceHolder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bfe872",
   "metadata": {},
   "source": [
    "### Sample bot with no memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94853668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that helps the user to plan an optimized itinerary.\"),\n",
    "    HumanMessage(content=\"I'm going to Rome for 2 days, what can I visit?\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chat(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da2e44",
   "metadata": {},
   "source": [
    "### Adding Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=chat, verbose=True, memory=memory\n",
    ")\n",
    "\n",
    "conversation.run(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20363387",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.run(\"what is the most iconic place in Rome?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.run(\"What kind of other events?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9863771",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40efc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagePlaceHolder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#LLM\n",
    "repo_id = 'databricks/dolly-v2-3b'\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id = repo_id, model_kwargs={\"temperature\":0.5, \"max_length\":1000}\n",
    ")\n",
    "\n",
    "#Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a helpful assistant that help the user to plan an optimized itinerary.\"\n",
    "        ),\n",
    "        # The`variable_name` here is what must align with memory\n",
    "        MessagePlaceHolder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_message=True)\n",
    "conversation = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ea5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input('you: ')\n",
    "    if query == 'q':\n",
    "        break\n",
    "    output = conversation({\"'question\": query})\n",
    "    print('User ', query)\n",
    "    print('AI System: ', output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2262d",
   "metadata": {},
   "source": [
    "### Adding non parametric knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a52553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "\n",
    "raw_documents = PyPDFLoader('italy_travel.pdf').load()\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2763de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key= 'chat_history',\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm,  retriever=db.as_retriever(), memory=memory, verbose=True)\n",
    "qa_chain.run({'question':'Give me some review about the Pantheon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cdad05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
