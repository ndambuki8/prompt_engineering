{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlqY4lLnJVU",
        "outputId": "e74bb010-94e7-4953-d2d9-339cecb23371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.4.26)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.0)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.81.0\n",
            "    Uninstalling openai-1.81.0:\n",
            "      Successfully uninstalled openai-1.81.0\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "%pip install openai==0.28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jsmgj7zwWL5"
      },
      "source": [
        "# Giving clear instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The principle of giving clear instructions is to provide the model with enough information and guidance\n",
        "to perform the task correctly and efficiently. Clear instructions should include the following elements:\n",
        "• The goal or objective of the task, such as “write a poem” or “summarize an article”\n",
        "• The format or structure of the expected output, such as “use four lines with rhyming words”\n",
        "or “use bullet points with no more than 10 words each”\n",
        "• The constraints or limitations of the task, such as “do not use any profanity” or “do not copy\n",
        "any text from the source”\n",
        "• The context or background of the task, such as “the poem is about autumn” or “the article is\n",
        "from a scientific journal”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4C4VErZfoWDr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "\n",
        "openai.api_key = \"API_KEY\"\n",
        "\n",
        "system_message = \"\"\"\n",
        "You are an AI assistant that helps humans by generating tutorials given a text.\n",
        "You will be provided with a text. If the text contains any kind of instructions on how to procees with something, generate a tutorial in a bullet list.\n",
        "Otherwise, inform the user that the text does not contain any instructions.\n",
        "\n",
        "Text:\n",
        "\"\"\"\n",
        "\n",
        "instructions = \"\"\"\n",
        "To prepare the known sauce from Genova, Italy, you can start by toasting the pine nuts to then coarsely chop them in a kitchen mortar and season with basil and garlic.\n",
        "Then, add half of the oil in the kitchen mortar and season with salt and pepper.\n",
        "Finally, transfer the pesto to a bowl and stir in the grated Parmesan cheese.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\", \"content\": system_message},\n",
        "        {\"role\":\"user\", \"content\":instructions}\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANImTSCAqkCD",
        "outputId": "ae4795ce-9f23-4198-e244-75c71d297d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- How to make Genovese sauce:\n",
            "    1. Start by toasting the pine nuts in a pan until they are lightly browned.\n",
            "    2. Coarsely chop the toasted pine nuts using a kitchen mortar and pestle.\n",
            "    3. Add basil leaves and garlic cloves to the mortar and continue to grind until well combined.\n",
            "    4. Gradually pour in half of the olive oil while mixing the ingredients in the mortar.\n",
            "    5. Season the mixture with salt and pepper to taste.\n",
            "    6. Transfer the pesto sauce from the mortar to a bowl.\n",
            "    7. Stir in the grated Parmesan cheese until well incorporated.\n",
            "    8. Your Genovese sauce is now ready to be served with your favorite pasta or dish. Enjoy!\n"
          ]
        }
      ],
      "source": [
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJhEofjzq_rw",
        "outputId": "4bce6ba3-3b00-4700-cef4-6066d05fe2c4"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdVtm2MWr3Bv",
        "outputId": "cd6bfca1-e1f6-4bad-9bb0-0b64dfd1fff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The text does not contain any instructions.\n"
          ]
        }
      ],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\", \"content\": system_message},\n",
        "        {\"role\":\"user\", \"content\":\"the sun is shining and dogs are running on the beach.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PopijnGxwaAg"
      },
      "source": [
        "# Split complex tasks into subtasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes, the tasks are too complex or ambiguous for a single prompt\n",
        "to handle, and it is better to split them into simpler subtasks that can be solved by different prompts.eg\n",
        "\n",
        "Machine translation: A complex task that involves translating a text from one language to\n",
        "another. This task can be split into subtasks such as:\n",
        "• Detecting the source language of the text\n",
        "• Converting the text into an intermediate representation that preserves the meaning\n",
        "and structure of the original text\n",
        "• Generating the text in the target language from the intermediate representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ccrPw11lxUK4"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "You are an AI assistant that summarizes articles.\n",
        "To complete this task, do the following subtasks:\n",
        "\n",
        "Read the provided article context comprehensively and identify the main topic and keypoints.\n",
        "Generate a paragraph summary of the current article context that captures the essential information and conveys the main idea.\n",
        "Print each step of the process.\n",
        "\n",
        "Article:\n",
        "\"\"\"\n",
        "\n",
        "article = \"\"\"\n",
        "Recurrent neural networks, long short-term memory and gated recurrent neural networks\n",
        "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
        "transduction problems such as language modeling and machine translation. Numerous\n",
        "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
        "architectures.\n",
        "Recurrent models typically factor computation along the symbol positions of the input and output\n",
        "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
        "states ht, as a function of the previous hidden state ht-1 and the input for position t. This inherently\n",
        "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
        "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
        "significant improvements in computational efficiency through factorization tricks and conditional\n",
        "computation, while also improving model performance in case of the latter. The fundamental\n",
        "constraint of sequential computation, however, remains.\n",
        "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
        "the input or output sequences. In all but a few cases, however, such attention mechanisms\n",
        "are used in conjunction with a recurrent network.\n",
        "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
        "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
        "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
        "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lZ0ETwr0wnHW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "openai.api_key = \"API_KEY\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\", \"content\": system_message},\n",
        "        {\"role\":\"user\", \"content\":article},\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmlrSrUOyER_",
        "outputId": "c19a09ed-7171-4fe5-dcdf-d491e55fdfd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Main Topic and Key Points\n",
            "Main Topic: The article discusses the use of recurrent neural networks, specifically long short-term memory and gated recurrent neural networks, in sequence modeling and transduction problems. It also introduces the Transformer model as an alternative approach that relies solely on an attention mechanism without recurrence.\n",
            "\n",
            "Key Points:\n",
            "- Recurrent neural networks are widely used in sequence modeling and transduction problems like language modeling and machine translation.\n",
            "- While recurrent models are effective, they face challenges in parallelization due to their sequential nature.\n",
            "- Attention mechanisms have enhanced sequence modeling by capturing dependencies without limitations on distance.\n",
            "- The article introduces the Transformer model, which uses attention mechanisms exclusively, allowing for more parallelization and achieving state-of-the-art translation quality in a relatively short training time.\n",
            "\n",
            "Step 2: Article Summary\n",
            "The article discusses the significance of recurrent neural networks, particularly long short-term memory and gated recurrent neural networks, in sequence modeling and transduction tasks like language modeling and machine translation. It highlights challenges faced by recurrent models in parallelization due to their sequential nature. Attention mechanisms are noted for improving modeling by capturing dependencies without distance limitations. Introducing the Transformer model, the article presents an alternative approach that relies solely on attention mechanisms for global dependency connections, allowing for increased parallelization and achieving high translation quality with shorter training times on multiple GPUs.\n"
          ]
        }
      ],
      "source": [
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSo3PwMDyk-j"
      },
      "source": [
        "# Asking for justification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LLMs are built in such a way that they predict the next token based on the previous ones without\n",
        "looking back at their generations. This might lead the model to output wrong content to the user, yet\n",
        "in a very convincing way. If the LLM-powered application does not provide a specific reference to\n",
        "that response, it might be hard to validate the ground truth behind it. Henceforth, specifying in the\n",
        "prompt to support the LLM’s answer with some reflections and justification could prompt the model to\n",
        "recover from its actions. Furthermore, asking for justification might be useful also in case of answers\n",
        "that are right but we simply don’t know the LLM’s reasoning behind it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSH-k45Ny0lK",
        "outputId": "29d7284d-84dd-45c4-fd57-2768c473c49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The answer to this riddle is a clock. \n",
            "\n",
            "A clock has a face with numbers or markings to indicate the time, and two hands (hour and minute hands) that move to show the current time. Although a clock has hands, it does not have arms or legs, making it the perfect solution to this riddle. \n",
            "\n",
            "The reasoning behind this answer is that the description aligns perfectly with the features of a clock - face, hands, but no arms or legs.\n"
          ]
        }
      ],
      "source": [
        "system_message = \"\"\"\n",
        "You are an AI assistant specialized in solving riddles.\n",
        "Given a riddle, solve it the best way you can.\n",
        "Provide a clear justification of your answer and the reasoning behind it.\n",
        "\n",
        "Riddle:\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "riddle = \"\"\"\n",
        "What has a face ans two hands, but no arms or legs?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\", \"content\":system_message},\n",
        "        {\"role\":\"user\", \"content\":riddle}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJXx39Kp0C6w"
      },
      "source": [
        "# Generate many outputs, then use the model to pick the best one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rather than generating just one response, we can prompt the model to generate multiple responses,\n",
        "and then pick the one that is most suitable for the user’s query. This splits the job into two subtasks\n",
        "for our LLM:\n",
        "1. Generating multiple responses to the user’s query\n",
        "2. Comparing those responses and picking the best one, according to some criteria we can specify\n",
        "in the metaprompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "You are an AI assistant specialized in solving riddles.\n",
        "Given a riddle, you have to generate three answers to the riddle.\n",
        "For each answer, be specific about the reasoning you made.\n",
        "Then among the three answers, select the one that is most plausible given the riddle.\n",
        "\n",
        "Riddle:\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "riddle = \"\"\"\n",
        "What has a face ans two hands, but no arms or legs?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\", \"content\":system_message},\n",
        "        {\"role\":\"user\", \"content\":riddle}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Repeat instructions at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LLMs tend not to process the metaprompt attributing the same weight or imprortance to all the sections. A way to overcome recency bias with prompt engineering techniques is to repeat the\n",
        "instructions or the main goal of the task at the end of the prompt. This can help remind the\n",
        "model of what it is supposed to do and what kind of response it should generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "You are a sentiment analyzer. You classsify conversations into three categories: positive, negative or neutral.\n",
        "Return only the sentiment, in lowercase and without punctuation.\n",
        "\n",
        "Conversation:\n",
        "{conversation}\n",
        "Remember to return only the sentiment, in lowercase and without punctuation.\n",
        "\"\"\"\n",
        "\n",
        "conversation = \"\"\"\n",
        "Customer: Hi, I need some help with my order.\n",
        "AI agent: Hello, welcome to our online store. I'm an AI agent and I'm here to assist you.\n",
        "Customer: I ordered a pair of shoes yesterday, but I haven't received a confirmation email yet. Can you check the status of my order?\n",
        "AI agent: Sure, I can help you with that. Can you please provide me with your order number and email address?\n",
        "Customer: Yes, my order number is 123456789 and my email is john.doe@example.com.\n",
        "AI agent: Thank you. I have found your order in our system. It looks like your order is still being processed and it will be shipped soon. You should receive a confirmation email within 24 hours.\n",
        "Customer: OK, thank you for the information. How long will it take for the shoes to arrive?\n",
        "AI agent: You're welcome. According to our shipping policy, it will take about 3 to 5 business days for the shoes to arrive at your address. You can track your order online using the tracking number that will be sent to your email once your order is shipped.\n",
        "Customer: Alright, sounds good. Thank you for your help.\n",
        "AI agent: It's my pleasure. Is there anything else I can do for you today?\n",
        "Customer: No, that's all. Have a nice day.\n",
        "AI agent: Thank you for choosing our online store. Have a nice day too. Goodbye.\n",
        "\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\", \"content\":system_message},\n",
        "        {\"role\":\"user\", \"content\":conversation},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using delimiters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## >>>> ====== ---- #######   ```````````"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A delimiter can be any sequence of characters or symbols that is clearly mapping a schema rather than a concept.\n",
        "This leads to a series of benefits, including:\n",
        "• Clear separation: Delimiters mark distinct sections within a prompt, separating instructions,\n",
        "examples, and desired output.\n",
        "• Guidance for LLMs: Proper use of delimiters removes ambiguity, guiding the model effectively.\n",
        "• Enhanced precision: Delimiters improve prompt understanding, resulting in more relevant\n",
        "responses.\n",
        "• Improved coherence: Effective use of delimiters organizes instructions, inputs, and outputs,\n",
        "leading to coherent responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "You are a Python expert who produces Python code as per the user's request.\n",
        "\n",
        "===>START EXAMPLE\n",
        "\n",
        "---User Query---\n",
        "Give me a function to print a string of text\n",
        "\n",
        "---User Output---\n",
        "Below you can find the described function:\n",
        "```def my_print(text):\n",
        "      return print(text)\n",
        "```\n",
        "\n",
        "<===END EXAMPLE\n",
        "\"\"\"\n",
        "\n",
        "query = \"generate a Python functino to calcuate the nth Fibonacci number\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\", \"content\":system_message},\n",
        "        {\"role\":\"user\", \"content\":query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Few-shot approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example, let’s say we want our model to generate a tagline for a new product line of climbing shoes\n",
        "we’ve just coined – Elevation Embrace. We have an idea of what the tagline should be like – concise\n",
        "and direct. We could explain it to the model in plain text; however, it might be more effective simply\n",
        "to provide it with some examples of similar projects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\"\n",
        "You are an AI marketing assistant. You help users to create taglines for new product names.\n",
        "Given a product name, produce a tagline similar to the following examples:\n",
        "\n",
        "Peak Pursuit - Conquer heights with comfort\n",
        "Summit Steps - Your Partner for Every Ascent\n",
        "Crag Conquerors - Step Up, Stand Tall\n",
        "\n",
        "\n",
        "Product name:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "product_name = 'Elevation Embrace'\n",
        "\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\", \"content\":system_message},\n",
        "        {\"role\":\"user\", \"content\":product_name}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### example2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "You are a binary classifier for sentiment analysis.\n",
        "Given a text, based on its sentiment you classify it into one of two categories: positive or negative.\n",
        "\n",
        "You can use the following texts as examples:\n",
        "\n",
        "Text: \"I love this product! It's fantastic and works perfectly.\"\n",
        "Positive\n",
        "\n",
        "Text: \"I'm really disappointed with the quality of the food.\"\n",
        "Negative\n",
        "\n",
        "Text: \"This is the best day of my life!\"\n",
        "Positive\n",
        "\n",
        "Text: \"I can't stand the noise in this restaurant.\"\n",
        "Negative\n",
        "\n",
        "ONLY return the sentiment as output (without punctuation).\n",
        "\n",
        "Text:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "text = \"The concert was amazing! The band was incredible!\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\", \"content\":system_message},\n",
        "        {\"role\":\"user\", \"content\":text}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### example3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as numpy\n",
        "import pandas as pandas\n",
        "\n",
        "df = pd.read_csv('movie.csv', encoding='utf-8')\n",
        "df['label'] = df['label'].replace({0:'Negative', 1: 'Positive'})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.sample(n=10, random_state=42)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "You are a binary classifier for sentiment analysis.\n",
        "Given a text, based on its sentiment you classify it into one of two categories: positive or negative.\n",
        "\n",
        "You can use the following texts as examples:\n",
        "\n",
        "Text: \"I love this product! It's fantastic and works perfectly.\"\n",
        "Positive\n",
        "\n",
        "Text: \"I'm really disappointed with the quality of the food.\"\n",
        "Negative\n",
        "\n",
        "Text: \"This is the best day of my life!\"\n",
        "Positive\n",
        "\n",
        "Text: \"I can't stand the noise in this restaurant.\"\n",
        "Negative\n",
        "\n",
        "ONLY return the sentiment as output (without punctuation).\n",
        "\n",
        "Text:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "text = \"The concert was amazing! The band was incredible!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_text(text):\n",
        "  response = openai.ChatCompletion.create(\n",
        "      engine=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "        {\"role\":\"system\", \"content\":system_message},\n",
        "        {\"role\":\"user\",\"content\":text}\n",
        "      ]\n",
        "  )\n",
        "  return response['choices'][0]['message']['content']\n",
        "\n",
        "df['predicted'] = df['text'].apply(process_text)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chain of Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduced in the paper Chain-of-Thought Prompting Elicits Reasoning in Large Language Models by\n",
        "Wei et al., chain of thought (CoT) is a technique that enables complex reasoning capabilities through\n",
        "intermediate reasoning steps. It also encourages the model to explain its reasoning, “forcing” it not\n",
        "to be too fast and risking giving the wrong response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "To solve a generic first-degree equation, follow these steps:\n",
        "\n",
        "1. **Identify the Equation:** Start by identifying the equation you want to solve. It should be in the form of \"ax + b = c,\" where 'a' is the coefficient of the variable, 'x' is the variable, 'b' is a constant, and 'c' is another constant.\n",
        "\n",
        "2. **Isolate the Variable:** Your goal is to isolate the variable 'x' on one side of the equation. To do this, perform the following steps:\n",
        "\n",
        "   a. **Add or Subtract Constants:** Add or subtract 'b' from both sides of the equation to move constants to one side.\n",
        "\n",
        "   b. **Divide by the Coefficient:** Divide both sides by 'a' to isolate 'x'. If 'a' is zero, the equation may not have a unique solution.\n",
        "\n",
        "3. **Simplify:** Simplify both sides of the equation as much as possible.\n",
        "\n",
        "4. **Solve for 'x':** Once 'x' is isolated on one side, you have the solution. It will be in the form of 'x = value.'\n",
        "\n",
        "5. **Check Your Solution:** Plug the found value of 'x' back into the original equation to ensure it satisfies the equation. If it does, you've found the correct solution.\n",
        "\n",
        "6. **Express the Solution:** Write down the solution in a clear and concise form.\n",
        "\n",
        "7. **Consider Special Cases:** Be aware of special cases where there may be no solution or infinitely many solutions, especially if 'a' equals zero.\n",
        "\n",
        "\n",
        "Equation:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "equation = \"3x + 5 = 11\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\", \"content\": system_message},\n",
        "        {\"role\":\"user\", \"content\": equation}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduced in the paper ReAct: Synergizing Reasoning and Acting in Language Models by Yao et al., ReAct\n",
        "(Reason and Act) is a general paradigm that combines reasoning and acting with LLMs. ReAct prompts\n",
        "the language model to generate verbal reasoning traces and actions for a task, and also receives observations from external sources such as web searches or databases. This allows the language model\n",
        "to perform dynamic reasoning and quickly adapt its action plan based on external information. For\n",
        "example, you can prompt the language model to answer a question by first reasoning about the question,\n",
        "then performing an action to send a query to the web, then receiving an observation from the search\n",
        "results, and then continuing with this thought, action, observation loop until it reaches a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.tools import BaseTool, StructuredTool, Tool\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model_name = 'gpt-3.5-turbo',\n",
        "    openai_api_key = openai.api_key\n",
        ")\n",
        "\n",
        "key = \"SERP_API_KEY\"\n",
        "\n",
        "search = SerpAPIWrapper(serpapi_api_key=key)\n",
        "tools = [\n",
        "    Tool.from_function(\n",
        "        func=search.run,\n",
        "        name=\"Search\",\n",
        "        description=\"useful for when you need to answer questions about current events\"\n",
        "    )\n",
        "]\n",
        "\n",
        "agent_executor = initialize_agent(tools, model, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(agent_executor.agent.llm_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_executor('who is most likely to win elections in Kenya 2027?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_executor('who is going to be WNBA MVP for the 2025 season?')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
